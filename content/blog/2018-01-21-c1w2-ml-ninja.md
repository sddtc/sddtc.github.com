---
title: Coursera 第一阶段第二课总结
date: 2018-01-21
tags: machine learning
---

### binary-classification 二元分类

* 深度学习在进行`m`个训练集计算的时候，其实并不需要使用`for`循环
* 神经网络的计算过程分为:**正向传播**和**反向传播**两个分开的过程
* 该课程主讲逻辑回归：用于二元分类的算法
* 识别猫咪的图片
	* RED GREEN BLUE 三个像素值矩阵
	* 定义一个`X`的向量矩阵成为特征向量
* 在二元分类问题中, 目标是训练出一个分类器
	* 输入: 上面的`X`
	* 输出: 结果标签`Y`:1\|0 

### logistic-regression 逻辑回归
* 是一个学习算法: 用在监督学习问题中, 用于二元分类问题
* 输出`Y’`值更像是一个概率, 趋近于1的`[0, 1)`
* 参数是`w`, `n_x`维向量, `b`是一个实数 这个公式??不太合理, 因为我们希望`Y’`在`[0, 1)`区间之内
* `sigmoid`函数出场了

### logistic-regression-cost-function 逻辑回归损失函数
* 为了训练`w`和`b`, 你需要定义一个成本函数
* 训练样本`m`个
* 损失函数(误差函数)用来衡量算法运行情况
* 函数不太合适的话, 会有优化问题, 会造成多个局部最优解, 那么运用梯度下降法会找不到全局最优解
* **损失函数**是针对**单个**训练样本
* **成本函数**是在**全体**训练样本上的表现

### gradient-descent 梯度下降法
* 用梯度下降法来训练或者学习训练集的`w`, `b`
* `w`其实是可以为高维
* 任意初始值都可以, 一般用`0`, `random`也可以但一般不用因为函数是凹的. 所以不论初始值是多少都应该能到达同一点
* 通过迭代朝**最陡**的下坡方向走一步
* 导数(斜率)就是针对`w`的导数, 即对参数`w`更新或者变化量
* `α`是学习率, 以后会学习, 它控制每一次迭代, 也可称作步长

### derivatives 导数
* 微积分 导数
* 对于一条直线 斜率永远都一样

### computation-graph 流程推导图
* 一个神经网络的计算都是按照**正向**或者**反向**过程传输操作
* **反向**一般用来计算对应的梯度或者导数
* 这个视频的流程图解释了**正向**和**反向**
* **从左向右**可以计算出函数`J`的结果
* **从右往左**是计算导数的最自然的方式, 下个视频介绍

### derivatives-with-a-computation-graph 流程推导图的导数计算
* 计算出函数`J`的导数
* 首先 计算`J`对`v`的导数, 也就做完了一个反向步
* 计算函数`J`对变量`a`的导数, 即`a`的改变量
* 计算函数`J`对变量`u`的导数

### logistic-regression-gradient-descent 逻辑回归的梯度下降法
* 怎样通过计算偏导数来实现逻辑回归的梯度下降算法
* 核心关键: 记住相关的几个重要公式来实现上面的算法
* 用流程图计算逻辑回归的梯度下降, 虽然大材小用但能更清晰的理解神经网络
* 链式法则 微积分

### gradient-descent-on-m-examples `m`个样本的梯度计算
* `m`个训练样本
* 记住逻辑回归的重要公式
* 初始化`0`, 循环`m`个求和
* 显式`for`低效
* **向量化技术**摆脱显式`for`循环, 在深度学习发展初期表现很棒, 可大大加速运算效率
* 深度学习里的向量化技术相当重要, 因为数据集越来越大, 需要高效的解决方案
* 甚至一个`for`都不需要

相关链接:
[网易云课堂-神经网络和深度学习](https://mooc.study.163.com/course/2001281002#/info)
[Coursera](https://www.coursera.org/specializations/deep-learning)
